{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea938c80-fb82-4b53-8f4e-5d483d009a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''1. Write a Spark code snippet to calculate the sum of a column in a DataFrame'''\n",
    "# Sample employee data\n",
    "from pyspark.sql.functions import col,sum\n",
    "data = [(\"John Doe\", \"john@example.com\", 50000.0),\n",
    "    (\"Jane Smith\", \"jane@example.com\", 60000.0),\n",
    "    (\"Bob Johnson\", \"bob@example.com\", 55000.0)]\n",
    "\n",
    "\n",
    "schema=\"Name string,email string,salary double\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "df_sum = df.agg(sum(\"salary\").alias(\"Total_salary\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83594558-2bfc-4420-b663-cbf428eb9be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(1,'Watson',34),(1,'Watson',40),(1,'Watson',34),(2,'Alex',45),(2,'Alex',50)]\n",
    "schema=\"ID int,Name string,Marks int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "from pyspark.sql.functions import collect_list,col\n",
    "df_list = df.groupBy(\"ID\",\"Name\").agg(collect_list(\"Marks\").alias(\"list_of_marks\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc846d5a-157b-473d-923e-559fb98bbae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Identify rows containing non-numeric values in the “Quantity” column, if any.\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "  StructField(\"ProductCode\", StringType(), True),\n",
    "  StructField(\"Quantity\", StringType(), True),\n",
    "  StructField(\"UnitPrice\", StringType(), True),\n",
    "  StructField(\"CustomerID\", StringType(), True),\n",
    "])\n",
    " \n",
    "\n",
    "data = [\n",
    "  (\"Q001\", 5, 20.0, \"C001\"),\n",
    "  (\"Q002\", 3, 15.5, \"C002\"),\n",
    "  (\"Q003\", 10, 5.99, \"C003\"),\n",
    "  (\"Q004\", 2, 50.0, \"C001\"),\n",
    "  (\"Q005\", \"nein\", 12.75, \"C002\"),\n",
    "]\n",
    " \n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "\n",
    "df_Filter = df.filter(col(\"Quantity\").rlike('^[a-zA-Z]*$')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434ba6d9-26b4-4ffb-8b44-dd00f5316255",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764150107404}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[('Paris','Polo, Tennis'),('Matt','Golf, Hockey'),('Sam',None)]\n",
    "schema=\"Person string,Games string\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "from pyspark.sql.functions import split,col,explode\n",
    "df_split = df.withColumn(\"Games\",split(col(\"Games\"),\",\"))\n",
    "df_explode = df_split.select(\"person\",explode(col(\"Games\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce70657-eda6-41aa-b0e0-e7a3fb291dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when,col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "data = [\n",
    " (1, \"Shipped\", \"2025-04-01\"),\n",
    " (1, \"Shipped\", \"2025-04-02\"),\n",
    " (2, \"Delivered\", \"2025-04-01\"),\n",
    " (2, \"Delivered\", \"2025-04-02\"),\n",
    " (2, \"Shipped\", \"2025-04-03\"),\n",
    " (3, \"Shipped\", \"2025-04-01\"),\n",
    " (3, \"Delivered\", \"2025-04-02\"),\n",
    " (3, \"Delivered\", \"2025-04-03\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    " StructField(\"order_id\", IntegerType(), False),\n",
    " StructField(\"order_status\", StringType(), True),\n",
    " StructField(\"order_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(data, schema)\n",
    "display(orders_df)\n",
    "\n",
    "\n",
    "df_order_count=orders_df.groupBy(\"order_id\").agg(count(\"*\").alias(\"total_orders\"))\n",
    "\n",
    "display(df_order_count)\n",
    "\n",
    "df_shipped_order_count=orders_df.filter(orders_df.order_status==\"Shipped\").groupBy(\"order_id\").agg(count(\"*\").alias(\"shipped_orders\")).display()\n",
    "\n",
    "df_delivered_order_count=orders_df.filter(orders_df.order_status==\"Delivered\").groupBy(\"order_id\").agg(count(\"*\").alias(\"delivered_orders\")).display()\n",
    "\n",
    "order_status_count=df_order_count.join(df_shipped_order_count,\"order_id\",\"left\").join(df_delivered_order_count,\"order_id\",\"left\").fillna(0)\n",
    "\n",
    "display(order_status_count)\n",
    "\n",
    "df_final=order_status_count.withColumn(\"shipped_percentage\",(col(\"shipped_orders\")/col(\"total_orders\"))*100)\\\n",
    "  .withColumn(\"delivered_percentage\",(col(\"delivered_orders\")/col(\"total_orders\"))*100).select(\"order_id\",\"shipped_percentage\",\"delivered_percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6589814-7dcf-4a34-8d1a-eb876e0e3d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"id\", IntegerType(), nullable=False),\n",
    "StructField(\"name\", StringType(), nullable=False), \n",
    "StructField(\"age\", IntegerType(), nullable=False),\n",
    "StructField(\"department\", StringType(), nullable=False), \n",
    "StructField(\"salary\", DoubleType(), nullable=False)\n",
    "])\n",
    "data = [\n",
    "    Row(1, \"John\", 30, \"Sales\", 50000.0),\n",
    "    Row(2, \"Alice\", 28, \"Marketing\", 60000.0),\n",
    "    Row(3, \"Bob\", 32, \"Finance\", 55000.0),\n",
    "    Row(4, \"Sarah\", 29, \"Sales\", 52000.0),\n",
    "    Row(5, \"Mike\", 31, \"Finance\", 58000.0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "\n",
    "df_group = df.groupBy(col(\"department\")).agg(sum(\"salary\")).alias(\"total_salary\").limit(1).display()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df_window = df.withColumn(\"rn\",row_number().over(w)).filter(col(\"rn\") == 1).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-26 14_50_49",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
