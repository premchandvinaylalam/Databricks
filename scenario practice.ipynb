{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fddec52-6d03-4556-b371-67784caf153d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,sum\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrames\").getOrCreate()\n",
    "Data = [(\"1\",\"prem@gmail.com\",30000),(\"2\",\"prem@yahoo.com\",40000),(\"3\",\"prem@hotmail.com\",50000)]\n",
    "schema = [\"id\",\"email\",\"salary\"]\n",
    "df = spark.createDataFrame(Data,schema)\n",
    "df.show()\n",
    "df1 = df.agg(sum(\"salary\").alias(\"total_sal\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0700a785-13c0-4d7f-bba1-3683fb9cfd93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(1,'Watson',34),(1,'Watson',40),(1,'Watson',34),(2,'Alex',45),(2,'Alex',50)]\n",
    "schema=\"ID int,Name string,Marks int\"\n",
    "df2=spark.createDataFrame(data,schema)\n",
    "display(df2)\n",
    "from pyspark.sql.functions import *\n",
    "df3 = df2.groupBy(\"ID\",\"Name\").agg(collect_list(\"Marks\").alias(\"List_of_marks\"))\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31372d47-1463-4594-9276-b100ca78790b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"quantity\",StringType(),True),StructField(\"price\",FloatType(),True),StructField(\"ProductCode\",StringType(),True)])\n",
    "data = [(\"100\",\"100.0\",\"A\"),(\"200\",\"200.0\",\"B\"),(\"300\",\"300.0\",\"C\"),(\"neon\",\"400.0\",\"D\")]\n",
    "df4 = spark.createDataFrame(data,schema)\n",
    "display(df4)\n",
    "df5 = df4.filter(col(\"productCode\").rlike('^[a-zA-Z]*$'))\n",
    "display(df5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0628bfd5-e45c-4b54-88fc-50b07236789d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"ProductCode\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"100\", 100.0, \"A\"),\n",
    "    (\"200\", 200.0, \"B\"),\n",
    "    (\"300\", 300.0, \"C\"),\n",
    "    (\"neon\", 400.0, \"D\")\n",
    "]\n",
    "\n",
    "df4 = spark.createDataFrame(data, schema)\n",
    "display(df4)\n",
    "\n",
    "df5 = df4.filter(col(\"quantity\").rlike('^[a-zA-Z]*$'))\n",
    "display(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302701b0-0bc3-4c8e-9615-b8445413ed9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(\"prem\",\"cricket,football\"),(\"chand\",\"hockey,chess\"),(\"vinay\",None)]\n",
    "schema = (\"name\",'games')\n",
    "df6 = spark.createDataFrame(data,schema)\n",
    "display(df6)\n",
    "df7 = df6.withColumn(\"games\",split(col('games'),','))\n",
    "display(df7)\n",
    "df7.select(\"name\",explode(col('games')).alias(\"games\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b022e2a-9775-48f6-bc7f-bcecaae1ac5e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764002515408}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(\"prem\",\"cricket:football:volleyball\"),(\"chand\",\"hockey:chess:carromboard\"),(\"vinay\",None)]\n",
    "schema = (\"name\",'games')\n",
    "df8 = spark.createDataFrame(data,schema)\n",
    "display(df8)\n",
    "df9 = df8.withColumn(\"games\",split(col('games'),':'))\n",
    "df9.select(\"name\",explode(col('games').alias(\"games\"))).display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572fed14-3893-4a76-89e2-b979e4e52f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "data=[(2025,1,'2025-01-01'),\n",
    "      (2025,1,'2025-01-02'),\n",
    "      (2025,1,'2025-01-03'),\n",
    "      (2025,1,'2025-01-04'),\n",
    "      (2025,1,'2025-01-05'),\n",
    "      (2025,1,'2025-01-06'),\n",
    "      (2025,1,'2025-01-07'),\n",
    "      (2025,2,'2025-01-08'),\n",
    "      (2025,2,'2025-01-09'),\n",
    "      (2025,2,'2025-01-10'),\n",
    "      (2025,2,'2025-01-11'),\n",
    "      (2025,2,'2025-01-12'),\n",
    "      (2025,2,'2025-01-13'),\n",
    "      (2025,2,'2025-01-14')]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"week_num\", IntegerType(), True),\n",
    "    StructField(\"dates\", StringType(), True)\n",
    "])\n",
    "\n",
    "df10 = spark.createDataFrame(data,schema)\n",
    "df10.groupBy(col('year'),col('week_num')).agg(min('dates').alias('start_week_date'),(max('dates').alias('end_week_date')))  .display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56bf41ba-edf6-4d35-ab89-34a9febac249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"id\", IntegerType(), nullable=False),\n",
    "StructField(\"name\", StringType(), nullable=False), \n",
    "StructField(\"age\", IntegerType(), nullable=False),\n",
    "StructField(\"department\", StringType(), nullable=False), \n",
    "StructField(\"salary\", DoubleType(), nullable=False)\n",
    "])\n",
    "data = [\n",
    "    Row(1, \"John\", 30, \"Sales\", 50000.0),\n",
    "    Row(2, \"Alice\", 28, \"Marketing\", 60000.0),\n",
    "    Row(3, \"Bob\", 32, \"Finance\", 55000.0),\n",
    "    Row(4, \"Sarah\", 29, \"Sales\", 52000.0),\n",
    "    Row(5, \"Mike\", 31, \"Finance\", 58000.0)\n",
    "]\n",
    "employeeDF = spark.createDataFrame(data, schema)\n",
    "display(employeeDF)\n",
    "df_groupby = employeeDF.groupBy('department').agg(sum('salary').alias('Total_salary'))\n",
    "display(df_groupby)\n",
    "df_window = Window.orderBy(col('Total_salary').desc())\n",
    "df_rownum = df_groupby.withColumn(\"rn\",row_number().over(df_window)).filter(col('rn') == 2)\n",
    "display(df_rownum)\n",
    "\n",
    "\n",
    "'''resultFD = employeeDF.groupBy('department').agg(sum('salary').alias('total_salary')).orderBy(col('total_salary').desc()).limit(1)\n",
    "display(resultFD)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b392ac9-09b8-4d10-9bee-298c19c9b389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_data = [(1,'Flight2' , 'Los Angeles' , 'London'),\n",
    "(1,'Flight1' , 'London' , 'Vatican'),\n",
    "(1,'Flight3' , 'Vatican' , 'Nantes'),\n",
    "(2,'Flight1' , 'Houston' , 'Paris'),\n",
    "(2,'Flight2' , 'Paris' , 'Nice')\n",
    "]\n",
    "\n",
    "schema = \"cust_id int, flight_id string , origin string , destination string\"\n",
    "flights_df = spark.createDataFrame(flights_data,schema)\n",
    "display(flights_df)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df_final1 = flights_df.orderBy('cust_id','flight_id').groupBy('cust_id').agg(first('origin').alias('origin'),(last('destination').alias('dest')))\n",
    "display(df_final1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d34e705-2ae6-4f84-a73f-8f6d0585d161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[('Paris','Polo, Tennis'),('Matt','Golf, Hockey'),('Sam',None)]\n",
    "schema=\"Person string,Games string\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "from pyspark.sql.functions import col,explode,split\n",
    "df_final = df.withColumn(\"Games\",split(col(\"Games\"),','))\n",
    "display(df_final)\n",
    "df_final.select(\"Person\",explode(col(\"Games\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825b2036-5786-4bca-b55a-8a3659413131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"id\", IntegerType(), nullable=False),\n",
    "StructField(\"name\", StringType(), nullable=False), \n",
    "StructField(\"age\", IntegerType(), nullable=False),\n",
    "StructField(\"department\", StringType(), nullable=False), \n",
    "StructField(\"salary\", DoubleType(), nullable=False)\n",
    "])\n",
    "data = [\n",
    "    Row(1, \"John\", 30, \"Sales\", 50000.0),\n",
    "    Row(2, \"Alice\", 28, \"Marketing\", 60000.0),\n",
    "    Row(3, \"Bob\", 32, \"Finance\", 55000.0),\n",
    "    Row(4, \"Sarah\", 29, \"Sales\", 52000.0),\n",
    "    Row(5, \"Mike\", 31, \"Finance\", 58000.0)\n",
    "]\n",
    "df = spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "df_final = df.groupBy(\"department\").agg(sum(\"salary\").alias(\"Total_salary\")).orderBy(col(\"Total_salary\").desc()).limit(1)\n",
    "display(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9626c086-ca88-4788-817c-bbc257f8f3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(1,'A',10),(2,'A',20),(1,'B',30),(2,'B',40)]\n",
    "schema=\"ID int,col1 string,col2 int\"\n",
    "df_pivot1=spark.createDataFrame(data,schema)\n",
    "display(df)\n",
    "df_pivot = df_pivot1.groupBy(col(\"ID\")).pivot(\"col1\").agg(first(\"col2\"))\n",
    "display(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa556bff-e059-4e45-b619-3b68ec88c78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"id\", IntegerType(), nullable=False),\n",
    "StructField(\"name\", StringType(), nullable=False), \n",
    "StructField(\"age\", IntegerType(), nullable=False),\n",
    "StructField(\"department\", StringType(), nullable=False), \n",
    "StructField(\"salary\", DoubleType(), nullable=False)\n",
    "])\n",
    "data = [\n",
    "    Row(1, \"John\", 30, \"Sales\", 50000.0),\n",
    "    Row(2, \"Alice\", 28, \"Marketing\", 60000.0),\n",
    "    Row(3, \"Bob\", 32, \"Finance\", 55000.0),\n",
    "    Row(4, \"Kevin\", 29, \"Sales\", 52000.0),\n",
    "    Row(5, \"Mike\", 31, \"Finance\", 58000.0)\n",
    "]\n",
    "employeeDF = spark.createDataFrame(data, schema)\n",
    "display(employeeDF)\n",
    "df_filter = employeeDF.filter(col(\"name\").startswith(\"J\")).agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "display(df_filter)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scenario practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
