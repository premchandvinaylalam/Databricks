{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fddec52-6d03-4556-b371-67784caf153d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,sum\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrames\").getOrCreate()\n",
    "Data = [(\"1\",\"prem@gmail.com\",30000),(\"2\",\"prem@yahoo.com\",40000),(\"3\",\"prem@hotmail.com\",50000)]\n",
    "schema = [\"id\",\"email\",\"salary\"]\n",
    "df = spark.createDataFrame(Data,schema)\n",
    "df.show()\n",
    "df1 = df.agg(sum(\"salary\").alias(\"total_sal\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0700a785-13c0-4d7f-bba1-3683fb9cfd93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(1,'Watson',34),(1,'Watson',40),(1,'Watson',34),(2,'Alex',45),(2,'Alex',50)]\n",
    "schema=\"ID int,Name string,Marks int\"\n",
    "df2=spark.createDataFrame(data,schema)\n",
    "display(df2)\n",
    "from pyspark.sql.functions import *\n",
    "df3 = df2.groupBy(\"ID\",\"Name\").agg(collect_list(\"Marks\").alias(\"List_of_marks\"))\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31372d47-1463-4594-9276-b100ca78790b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"quantity\",StringType(),True),StructField(\"price\",FloatType(),True),StructField(\"ProductCode\",StringType(),True)])\n",
    "data = [(\"100\",\"100.0\",\"A\"),(\"200\",\"200.0\",\"B\"),(\"300\",\"300.0\",\"C\"),(\"neon\",\"400.0\",\"D\")]\n",
    "df4 = spark.createDataFrame(data,schema)\n",
    "display(df4)\n",
    "df5 = df4.filter(col(\"productCode\").rlike('^[a-zA-Z]*$'))\n",
    "display(df5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0628bfd5-e45c-4b54-88fc-50b07236789d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"ProductCode\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"100\", 100.0, \"A\"),\n",
    "    (\"200\", 200.0, \"B\"),\n",
    "    (\"300\", 300.0, \"C\"),\n",
    "    (\"neon\", 400.0, \"D\")\n",
    "]\n",
    "\n",
    "df4 = spark.createDataFrame(data, schema)\n",
    "display(df4)\n",
    "\n",
    "df5 = df4.filter(col(\"quantity\").rlike('^[a-zA-Z]*$'))\n",
    "display(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302701b0-0bc3-4c8e-9615-b8445413ed9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(\"prem\",\"cricket,football\"),(\"chand\",\"hockey,chess\"),(\"vinay\",None)]\n",
    "schema = (\"name\",'games')\n",
    "df6 = spark.createDataFrame(data,schema)\n",
    "display(df6)\n",
    "df7 = df6.withColumn(\"games\",split(col('games'),','))\n",
    "display(df7)\n",
    "df7.select(\"name\",explode(col('games')).alias(\"games\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b022e2a-9775-48f6-bc7f-bcecaae1ac5e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764002515408}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(\"prem\",\"cricket:football:volleyball\"),(\"chand\",\"hockey:chess:carromboard\"),(\"vinay\",None)]\n",
    "schema = (\"name\",'games')\n",
    "df8 = spark.createDataFrame(data,schema)\n",
    "display(df8)\n",
    "df9 = df8.withColumn(\"games\",split(col('games'),':'))\n",
    "df9.select(\"name\",explode(col('games').alias(\"games\"))).display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572fed14-3893-4a76-89e2-b979e4e52f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "data=[(2025,1,'2025-01-01'),\n",
    "      (2025,1,'2025-01-02'),\n",
    "      (2025,1,'2025-01-03'),\n",
    "      (2025,1,'2025-01-04'),\n",
    "      (2025,1,'2025-01-05'),\n",
    "      (2025,1,'2025-01-06'),\n",
    "      (2025,1,'2025-01-07'),\n",
    "      (2025,2,'2025-01-08'),\n",
    "      (2025,2,'2025-01-09'),\n",
    "      (2025,2,'2025-01-10'),\n",
    "      (2025,2,'2025-01-11'),\n",
    "      (2025,2,'2025-01-12'),\n",
    "      (2025,2,'2025-01-13'),\n",
    "      (2025,2,'2025-01-14')]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"week_num\", IntegerType(), True),\n",
    "    StructField(\"dates\", StringType(), True)\n",
    "])\n",
    "\n",
    "df10 = spark.createDataFrame(data,schema)\n",
    "df10.groupBy(col('year'),col('week_num')).agg(min('dates').alias('start_week_date'),(max('dates').alias('end_week_date')))  .display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scenario practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
